{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"cbb7d02b-70b7-4cd2-b6fd-30e67bb86742","cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\nimport re\n\n# **Device Configuration**\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Using device: {device}\")\n\n# **Email Body Cleaning Function**\ndef clean_email_body(body):\n    # Remove quoted replies (lines starting with \">\")\n    body = re.sub(r\"(^|\\n)>.*\", \"\", body)\n    # Remove excessive asterisks and separator lines\n    body = re.sub(r\"\\*{2,}.*\", \"\", body)\n    # Remove trailing email artifacts and forward markers\n    body = re.sub(r\"(\\n-----.*|----- Forwarded by.*|----------------------.*)\", \"\", body)\n    # Remove email signatures and disclaimers\n    body = re.sub(r\"(--\\\\s*|\\\\nRegards,|\\\\nBest,|\\\\nSincerely,).*\", \"\", body, flags=re.IGNORECASE)\n    body = re.sub(r\"(Disclaimer:|This e-mail is the property of).*\", \"\", body, flags=re.IGNORECASE)\n    # Replace multiple newlines with a single space\n    body = re.sub(r\"\\s+\", \" \", body).strip()\n    return body\n    \n# **Simplify Metadata for Each Email**\ndef simplify_metadata(row):\n    # Fix issues with list parsing in \"To\"\n    recipients = \", \".join(row[\"to\"]) if isinstance(row[\"to\"], list) else row[\"to\"]\n    return f\"From: {row['from']} To: {recipients} Subject: {row['subject']} Body: {clean_email_body(row['body'])}\"\n\n# **Preprocessing Function**\ndef preprocess_threads_optimized(df):\n    grouped = df.groupby(\"thread_id\").apply(lambda x: {\n        \"input_text\": \" \".join(\n            pd.Series([\n                simplify_metadata(row)  # Format each email\n                for _, row in x.iterrows()\n            ]).drop_duplicates().tolist()\n        ),\n        \"summary\": x[\"summary\"].iloc[0]\n    }).reset_index(drop=True)\n    return pd.DataFrame(grouped.tolist())\n\n# **Load Dataset**\nfrom datasets import load_dataset\nds = load_dataset(\"xprilion/email-summary-dataset\")\ndf = pd.DataFrame(ds[\"train\"])\n\n# **Apply Preprocessing**\npreprocessed_data = preprocess_threads_optimized(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:45:30.798979Z","iopub.execute_input":"2025-01-20T19:45:30.799340Z","iopub.status.idle":"2025-01-20T19:45:44.354001Z","shell.execute_reply.started":"2025-01-20T19:45:30.799312Z","shell.execute_reply":"2025-01-20T19:45:44.353186Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261bb0919b854ee284108f96b8e5c23e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.csv:   0%|          | 0.00/50.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2dae0464b134556b6fa70f2da4b96ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/21684 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f69fbb591a41038e6faf875cf5a67b"}},"metadata":{}}],"execution_count":6},{"id":"46f81aed-516c-420e-b3d1-3b68aea5e616","cell_type":"code","source":"# **Validate Cleaning and Formatting**\nprint(\"Sample processed input_text:\")\nprint(preprocessed_data[\"input_text\"].sample(2, random_state=42).values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:45:44.354994Z","iopub.execute_input":"2025-01-20T19:45:44.355594Z","iopub.status.idle":"2025-01-20T19:45:44.362077Z","shell.execute_reply.started":"2025-01-20T19:45:44.355569Z","shell.execute_reply":"2025-01-20T19:45:44.361323Z"}},"outputs":[{"name":"stdout","text":"Sample processed input_text:\n['From: Eric Bass To: [\\'Phillip M Love\\'] Subject: Trade Body: I offered you my 1 and 3 for your 1 and 4. I can throw in my 5 for your 6. Let me know. Eric From: Eric Bass To: [\\'Jason.Bass2@COMPAQ.com\\'] Subject: Trade Body: The 24th (2) pick and the 42nd (4) pick for your 2nd round and your 7th round? From: Eric Bass To: [\\'lqcolombo@aol.com\\'] Subject: Trade Body: Muhsin Muhammed and Elvis Grbac for Jeff Garcia, Raymont Harris, and Wayne Chrebet? Gives you starting QB and depth at RB (all 4 of your RBs will likely not play week 1 and maybe 2). Let me know From: Eric Bass To: [\\'Jason.Bass2@COMPAQ.com\\'] Subject: Trade Body: Hear anything on Engram? From: Eric Bass To: [\"O\\'Neal D Winfree\"] Subject: Trade Body: Are you going to put it in the system or do you just want me to do it? From: Eric Bass To: [\\'Steve Venturatos\\'] Subject: Trade Body: Are we done on that trade? Richardson, Bettis, and Chrebet for Freeman 3 Starters for 1 From: Eric Bass To: [\\'Steve Venturatos\\'] Subject: Trade Body: I will give you Bill Schroeder/Darnell Autry and Kyle Brady for Frank Wycheck From: Eric Bass To: [\\'Jason.Bass2@COMPAQ.com\\'] Subject: Trade Body: Mcnabb and Freeman for Bruce'\n \"From: Sally Beck To: ['Patti Thompson'] Subject: Peoples presentation Body: Please print in color for me. Thanks. Robert Superty 04/05/2001 04:53 PM To: Sally Beck/HOU/ECT@ECT, Bob M Hall/NA/Enron@Enron cc: Subject: Peoples presentation Sally, Bob FYI (or your files) - here is the presentation I gave to the Peoples Energy Board of Directors. Let me know if you have any questions - Bob From: Sally Beck To: ['Rahil Jafry', 'Greg Piper'] Subject: Peoples presentation Body: This is a copy of the presentation that Bob Superty made to the Peoples Energy Board this week. Lousie Kitchen told me that he was very impressive in the presentation (not a surprise to me, but it is nice when others recognize the talent that is in the operations organization). We may be able to incorporate some of this into our presentation to originators for internal purposes, and there may be parts of this that could be used in external marketing tools. --Sally Robert Superty 04/05/2001 04:53 PM To: Sally Beck/HOU/ECT@ECT, Bob M Hall/NA/Enron@Enron cc: Subject: Peoples presentation Sally, Bob FYI (or your files) - here is the presentation I gave to the Peoples Energy Board of Directors. Let me know if you have any questions - Bob From: Sally Beck To: ['Bob M Hall'] Subject: Peoples presentation Body: In a meeting with Louise Kitchen today, she mentioned that Bob Superty was very impressive in a presentation that he made to the Peoples Energy Board of Directors on Wednesday. She was very complimentary. I have let Bob know about Louise's praise. Thought that you would like to know as well. --Sally Robert Superty 04/05/2001 04:53 PM To: Sally Beck/HOU/ECT@ECT, Bob M Hall/NA/Enron@Enron cc: Subject: Peoples presentation Sally, Bob FYI (or your files) - here is the presentation I gave to the Peoples Energy Board of Directors. Let me know if you have any questions - Bob From: Sally Beck To: ['Patti Thompson <Patti Thompson/HOU/ECT@ECT'] Subject: Peoples presentation Body: Please print in color for me. Thanks. Robert Superty 04/05/2001 04:53 PM To: Sally Beck/HOU/ECT@ECT, Bob M Hall/NA/Enron@Enron cc: Subject: Peoples presentation Sally, Bob FYI (or your files) - here is the presentation I gave to the Peoples Energy Board of Directors. Let me know if you have any questions - Bob From: Sally Beck To: ['Rahil Jafry <Rahil Jafry/HOU/ECT@ECT', 'Greg Piper <Greg Piper/Corp/Enron@ECT'] Subject: Peoples presentation Body: This is a copy of the presentation that Bob Superty made to the Peoples Energy Board this week. Lousie Kitchen told me that he was very impressive in the presentation (not a surprise to me, but it is nice when others recognize the talent that is in the operations organization). We may be able to incorporate some of this into our presentation to originators for internal purposes, and there may be parts of this that could be used in external marketing tools. --Sally Robert Superty 04/05/2001 04:53 PM To: Sally Beck/HOU/ECT@ECT, Bob M Hall/NA/Enron@Enron cc: Subject: Peoples presentation Sally, Bob FYI (or your files) - here is the presentation I gave to the Peoples Energy Board of Directors. Let me know if you have any questions - Bob From: Sally Beck To: ['Bob M Hall <Bob M Hall/NA/Enron@Enron'] Subject: Peoples presentation Body: In a meeting with Louise Kitchen today, she mentioned that Bob Superty was very impressive in a presentation that he made to the Peoples Energy Board of Directors on Wednesday. She was very complimentary. I have let Bob know about Louise's praise. Thought that you would like to know as well. --Sally Robert Superty 04/05/2001 04:53 PM To: Sally Beck/HOU/ECT@ECT, Bob M Hall/NA/Enron@Enron cc: Subject: Peoples presentation Sally, Bob FYI (or your files) - here is the presentation I gave to the Peoples Energy Board of Directors. Let me know if you have any questions - Bob\"]\n","output_type":"stream"}],"execution_count":7},{"id":"9fc9d49a-1a0d-49ae-a4d4-612eb702f391","cell_type":"code","source":"# Split data\ntrain_size = int(0.8 * len(preprocessed_data))\ntrain_data = preprocessed_data[:train_size]\ntest_data = preprocessed_data[train_size:]\n\n# Initialize tokenizer\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:45:50.924410Z","iopub.execute_input":"2025-01-20T19:45:50.924713Z","iopub.status.idle":"2025-01-20T19:45:51.971851Z","shell.execute_reply.started":"2025-01-20T19:45:50.924687Z","shell.execute_reply":"2025-01-20T19:45:51.970999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80adb2801d4d4da09d1bb7df628e45f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd192998fd1402c84cc9f657e6a8d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3e7da7175ff4fac97c1adb23f4ce151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c897379aa05d4e38ad480f3b01ddd14e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"id":"98409497-c99a-4016-a7ea-18c9d2630f3e","cell_type":"code","source":"# Dataset Class\nclass EmailDataset(Dataset):\n    def __init__(self, data, tokenizer, max_input_length=1024, max_summary_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_summary_length = max_summary_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # Get single example\n        item = self.data.iloc[idx]\n        \n        # Tokenize input and summary on-the-fly\n        inputs = self.tokenizer(\n            item['input_text'],\n            max_length=self.max_input_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        labels = self.tokenizer(\n            item['summary'],\n            max_length=self.max_summary_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        # Remove batch dimension added by tokenizer\n        return {\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'labels': labels['input_ids'].squeeze(0)\n        }\n    \n    @staticmethod\n    def collate_fn(batch):\n        # Combine batch elements efficiently\n        batch_inputs = {\n            'input_ids': torch.stack([x['input_ids'] for x in batch]),\n            'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n            'labels': torch.stack([x['labels'] for x in batch])\n        }\n        return batch_inputs\n\n# Create datasets\ntrain_dataset = EmailDataset(train_data, tokenizer)\ntest_dataset = EmailDataset(test_data, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:47:29.847204Z","iopub.execute_input":"2025-01-20T19:47:29.847547Z","iopub.status.idle":"2025-01-20T19:47:29.856053Z","shell.execute_reply.started":"2025-01-20T19:47:29.847521Z","shell.execute_reply":"2025-01-20T19:47:29.854797Z"}},"outputs":[],"execution_count":12},{"id":"cb8827d5-7817-4000-a0d1-7143923a9933","cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm import tqdm\nimport os\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Ensure GPU \nis used efficiently\ntorch.cuda.empty_cache()\n\n# Create dataloaders\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=8, \n    shuffle=True,\n    collate_fn=EmailDataset.collate_fn\n)\n\ntest_dataloader = DataLoader(\n    test_dataset, \n    batch_size=8,\n    collate_fn=EmailDataset.collate_fn\n)\n\n# Model and Optimizer\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Initialize optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n\n# Early Stopping Parameters\npatience = 2\nbest_loss = float(\"inf\")\nepochs_without_improvement = 0\n\n# Training Loop\nnum_epochs = 10\ngradient_accumulation_steps = 4\n\n# Create directory for saving model\nos.makedirs(\"./saved_model\", exist_ok=True)\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    num_batches = 0\n    \n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    \n    # Initialize progress bar\n    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n    \n    for step, batch in enumerate(progress_bar):\n        # Move batch to device\n        batch = {key: value.to(device) for key, value in batch.items()}\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        \n        # Compute loss\n        loss = outputs.loss\n        if loss.ndimension() > 0:\n            loss = loss.mean()\n        \n        # Gradient accumulation\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n        \n        if (step + 1) % gradient_accumulation_steps == 0:\n            # Gradient clipping (optional)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\"loss\": loss.item() * gradient_accumulation_steps})\n        \n        # Accumulate loss\n        epoch_loss += loss.item() * gradient_accumulation_steps\n        num_batches += 1\n    \n    # Step the learning rate scheduler once per epoch\n    lr_scheduler.step()\n    \n    # Calculate average loss for the epoch\n    avg_epoch_loss = epoch_loss / num_batches\n    print(f\"\\nEpoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n    \n    # Early stopping check\n    if avg_epoch_loss < best_loss:\n        best_loss = avg_epoch_loss\n        epochs_without_improvement = 0\n        print(f\"New best loss achieved: {best_loss:.4f}\")\n        \n        # Save the best model\n        try:\n            if isinstance(model, torch.nn.DataParallel):\n                model.module.save_pretrained(\"./saved_model\")\n            else:\n                model.save_pretrained(\"./saved_model\")\n            tokenizer.save_pretrained(\"./saved_model\")\n            print(\"Model saved successfully\")\n        except Exception as e:\n            print(f\"Error saving model: {e}\")\n    else:\n        epochs_without_improvement += 1\n        \n    if epochs_without_improvement >= patience:\n        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n        break\n\nprint(\"\\nTraining completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:05:21.765000Z","iopub.execute_input":"2025-01-20T10:05:21.765229Z","iopub.status.idle":"2025-01-20T12:50:15.406052Z","shell.execute_reply.started":"2025-01-20T10:05:21.765210Z","shell.execute_reply":"2025-01-20T12:50:15.405074Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c891d8efb24e34a206e1764dc16171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6f153754254f7faa8e8bb869f28508"}},"metadata":{}},{"name":"stdout","text":"Using 2 GPUs!\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 1: 100%|██████████| 417/417 [16:22<00:00,  2.36s/it, loss=1.19] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Average Loss: 1.6555\nNew best loss achieved: 1.6555\n","output_type":"stream"},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Model saved successfully\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 2: 100%|██████████| 417/417 [16:24<00:00,  2.36s/it, loss=1.28] \nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Average Loss: 1.2339\nNew best loss achieved: 1.2339\nModel saved successfully\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 3: 100%|██████████| 417/417 [16:24<00:00,  2.36s/it, loss=0.948]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Average Loss: 1.1097\nNew best loss achieved: 1.1097\nModel saved successfully\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 4: 100%|██████████| 417/417 [16:24<00:00,  2.36s/it, loss=1.34] \nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Average Loss: 1.0077\nNew best loss achieved: 1.0077\nModel saved successfully\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 5: 100%|██████████| 417/417 [16:23<00:00,  2.36s/it, loss=0.915]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Average Loss: 0.9241\nNew best loss achieved: 0.9241\nModel saved successfully\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 6: 100%|██████████| 417/417 [16:24<00:00,  2.36s/it, loss=0.92] \nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 Average Loss: 0.8540\nNew best loss achieved: 0.8540\nModel saved successfully\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 7: 100%|██████████| 417/417 [16:23<00:00,  2.36s/it, loss=0.91] \nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 Average Loss: 0.7872\nNew best loss achieved: 0.7872\nModel saved successfully\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 8: 100%|██████████| 417/417 [16:23<00:00,  2.36s/it, loss=0.756]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 Average Loss: 0.7284\nNew best loss achieved: 0.7284\nModel saved successfully\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 9: 100%|██████████| 417/417 [16:23<00:00,  2.36s/it, loss=1.2]  \nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 Average Loss: 0.6777\nNew best loss achieved: 0.6777\nModel saved successfully\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10:   0%|          | 0/417 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 10: 100%|██████████| 417/417 [16:23<00:00,  2.36s/it, loss=0.617]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 Average Loss: 0.6324\nNew best loss achieved: 0.6324\nModel saved successfully\n\nTraining completed!\n","output_type":"stream"}],"execution_count":5},{"id":"9302965f-79dd-46d8-b921-f5d8e63e5f8a","cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizer, GenerationConfig\nimport torch\n\n# Load the saved model and tokenizer\nmodel_path = \"./saved_model\"\n\nmodel = BartForConditionalGeneration.from_pretrained(model_path)\ntokenizer = BartTokenizer.from_pretrained(model_path)\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Load generation config if previously saved\ntry:\n    generation_config = GenerationConfig.from_pretrained(model_path)\nexcept:\n    # Default generation config if not saved\n    generation_config = GenerationConfig(\n        max_length=142,\n        min_length=56,\n        early_stopping=True,\n        num_beams=4,\n        length_penalty=2.0,\n        no_repeat_ngram_size=3\n    )\n\n# Function to generate a summary\ndef generate_summary(text):\n    # Tokenize input text\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"longest\").to(device)\n\n    # Generate summary\n    with torch.no_grad():\n        summary_ids = model.generate(\n            inputs[\"input_ids\"],\n            generation_config=generation_config\n        )\n\n    # Decode the generated summary\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T12:50:15.407396Z","iopub.execute_input":"2025-01-20T12:50:15.407742Z","iopub.status.idle":"2025-01-20T12:50:16.923053Z","shell.execute_reply.started":"2025-01-20T12:50:15.407711Z","shell.execute_reply":"2025-01-20T12:50:16.922165Z"}},"outputs":[],"execution_count":6},{"id":"48fbf411-b569-4bdd-b03b-88be314a1604","cell_type":"code","source":"# Example text\nexample_text = \"\"\"\nSubject: Mandatory Participation Requirements for NSO Credits\n    From: NSO Secretary\n    To: Students, Mahavir, DOSA\n    \n    Body:\n    Dear Students,\n\nWelcome to the NSO program for this semester. Please note the inclusion of a mandatory component—the Run for Wellbeing (R4W) Event—as part of the credit requirements. Below are the detailed guidelines:\n\nKey Requirements for NSO Credits\nFor students registering for 2 credits (28 hours):\n10 hours must be completed through participation in 10 R4W sessions (1 session = 1 hour).\nEach R4W session involves a 5 km run or walk.\nAttendance in a minimum of 10 sessions is mandatory.\nThe remaining 18 hours must be completed through other sports activities organized during the semester.\n\nImportant Grading Guidelines:\n\nCompleting 10 R4W + 18 sports = 28 hours will guarantee ‘Satisfactory’ S grades for both credits (Grade: SS).\nFailing to complete 10 R4W sessions will result in at least one ‘Unsatisfactory’ X grade (Grades: XS/SX/XX).\nFailing to complete 5 R4W sessions will result in ‘Unsatisfactory’ X grades in both credits (Grade: XX).\nFor students registering for 1 credit (14 hours):\n5 hours must be completed through 5 R4W sessions.\nThe remaining 9 hours must be covered through other sports activities to guarantee an S grade.\nR4W Calendar\n\nThe schedule of R4W sessions is attached to this email. Ensure you plan your participation accordingly to avoid any last-minute challenges.\n\nTracking and Attendance\nTrack your activity using the Strava app.\nDownload link: Strava App\nTake a screenshot of your completed activity on Strava, displaying the distance and time.\nSubmit your screenshot via a Google form that will be shared after the event to mark your attendance.\n\nFor any queries or clarifications, please feel free to contact us.\n\nBest regards,\n\n\n\n--\n\n\n\nAKSHAT KUMAR AND BUBLI BRAHMA  \n\nNSO Secretaries\nIndian Institute of Technology, Bhilai     \nContact No. 7737288510, 6002846132\n\"\"\"\n\n# Generate and print the summary\nsummary = generate_summary(example_text)\nprint(\"Generated Summary:\")\nprint(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T13:10:47.499789Z","iopub.execute_input":"2025-01-20T13:10:47.500109Z","iopub.status.idle":"2025-01-20T13:10:49.402534Z","shell.execute_reply.started":"2025-01-20T13:10:47.500082Z","shell.execute_reply":"2025-01-20T13:10:49.401815Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Summary:\nThe Run for Wellbeing (R4W) event is a mandatory component of the NSO program for students registering for two credits (28 hours). Each R4W session involves a 5 km run or walk, and attendance in a minimum of 10 sessions is mandatory. The remaining 18 hours must be completed through other sports activities during the semester. Failing to complete these sessions will result in unsatisfactory X grades in both credits. Tracking and attendance are provided, and students can also use the Strava app to track their activity and take a screenshot to mark their attendance.\n","output_type":"stream"}],"execution_count":7},{"id":"6037225f-be01-461e-8760-dd7513cf6851","cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:47:04.793366Z","iopub.execute_input":"2025-01-20T19:47:04.793655Z","iopub.status.idle":"2025-01-20T19:47:09.247642Z","shell.execute_reply.started":"2025-01-20T19:47:04.793634Z","shell.execute_reply":"2025-01-20T19:47:09.246807Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":10},{"id":"d37d19c8-a142-4fd6-9b2a-5c20dbe6accc","cell_type":"code","source":"from rouge import Rouge\n\n# Load model and tokenizer\nmodel_path = \"/kaggle/working/saved_model/\"  # Path to your saved model\nmodel = BartForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\ntokenizer = BartTokenizer.from_pretrained(model_path)\n\n# Create DataLoader for test dataset\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=8,  # Adjust batch size based on GPU memory\n    collate_fn=EmailDataset.collate_fn,\n    shuffle=False\n)\n\n# Function to generate summaries\ndef generate_summary_from_dataloader(dataloader, model, tokenizer, device=\"cuda\"):\n    model.eval()\n    generated_summaries = []\n    reference_summaries = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            # Move batch to device\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            # Generate summaries\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,  # Adjust max_length as needed\n                min_length=30,\n                num_beams=4,\n                length_penalty=2.0\n            )\n            \n            # Decode generated summaries and references\n            generated_summaries.extend(\n                tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            )\n            reference_summaries.extend(\n                tokenizer.batch_decode(labels, skip_special_tokens=True)\n            )\n    \n    return generated_summaries, reference_summaries\n\n# Generate and evaluate summaries\ngenerated_summaries, reference_summaries = generate_summary_from_dataloader(\n    test_dataloader, model, tokenizer\n)\n\n# Calculate ROUGE scores\nrouge = Rouge()\nscores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n\n# Print the ROUGE scores\nprint(\"ROUGE-1:\", scores[\"rouge-1\"])\nprint(\"ROUGE-2:\", scores[\"rouge-2\"])\nprint(\"ROUGE-L:\", scores[\"rouge-l\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:47:46.012080Z","iopub.execute_input":"2025-01-20T19:47:46.012448Z","iopub.status.idle":"2025-01-20T20:04:29.216345Z","shell.execute_reply.started":"2025-01-20T19:47:46.012420Z","shell.execute_reply":"2025-01-20T20:04:29.215461Z"}},"outputs":[{"name":"stdout","text":"ROUGE-1: {'r': 0.4973023626945872, 'p': 0.4503773073674005, 'f': 0.46858544538854363}\nROUGE-2: {'r': 0.24413624391534525, 'p': 0.21660355647022325, 'f': 0.2271890926861715}\nROUGE-L: {'r': 0.46844449909896385, 'p': 0.4245539374534186, 'f': 0.44157005926890974}\n","output_type":"stream"}],"execution_count":13},{"id":"7a54b9ee-9c2a-4b78-947e-f141b4f1a9e5","cell_type":"code","source":"import torch\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\n# Load your fine-tuned BART model and tokenizer\nmodel_name = \"/kaggle/working/saved_model/\"  # Replace with your model path\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\ntokenizer = BartTokenizer.from_pretrained(model_name)\n\ndef generate_streaming(text, max_length=142, min_length=56, num_beams=4, length_penalty=2.0, no_repeat_ngram_size=3):\n    # Tokenize the input\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n\n    \n\n    # Generate token IDs incrementally\n    generated_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=max_length,\n        min_length=min_length,\n        early_stopping=True,\n        num_beams=num_beams,\n        length_penalty=length_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        output_scores=False,  # Scores are not needed for streaming\n        return_dict_in_generate=True,\n    )\n\n    # Decode and stream tokens\n    generated_tokens = []\n    for token_id in generated_ids.sequences[0]:\n        generated_tokens.append(token_id.item())  # Convert token ID to Python int\n        current_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        print(current_output, end=\"\\r\", flush=True)  # Display the current output incrementally\n        yield current_output  # Optionally send the output to an external UI/frontend\n\n# Example text\nexample_text = \"\"\"\nSubject: Mandatory Participation Requirements for NSO Credits\nFrom: NSO Secretary\nTo: Students, Mahavir, DOSA\n\nBody:\nDear Students,\n\nWelcome to the NSO program for this semester. Please note the inclusion of a mandatory component—the Run for Wellbeing (R4W) Event—as part of the credit requirements. Below are the detailed guidelines:\n\nKey Requirements for NSO Credits:\n1. For students registering for 2 credits (28 hours):\n   - 10 hours must be completed through participation in 10 R4W sessions (1 session = 1 hour).\n   - Each R4W session involves a 5 km run or walk.\n   - Attendance in a minimum of 10 sessions is mandatory.\n   - The remaining 18 hours must be completed through other sports activities organized during the semester.\n\n2. Important Grading Guidelines:\n   - Completing 10 R4W + 18 sports = 28 hours will guarantee ‘Satisfactory’ S grades for both credits (Grade: SS).\n   - Failing to complete 10 R4W sessions will result in at least one ‘Unsatisfactory’ X grade (Grades: XS/SX/XX).\n   - Failing to complete 5 R4W sessions will result in ‘Unsatisfactory’ X grades in both credits (Grade: XX).\n\n3. For students registering for 1 credit (14 hours):\n   - 5 hours must be completed through 5 R4W sessions.\n   - The remaining 9 hours must be covered through other sports activities to guarantee an S grade.\n\nR4W Calendar:\nThe schedule of R4W sessions is attached to this email. Ensure you plan your participation accordingly to avoid any last-minute challenges.\n\nTracking and Attendance:\n- Track your activity using the Strava app. Download link: [Strava App]\n- Take a screenshot of your completed activity on Strava, displaying the distance and time.\n- Submit your screenshot via a Google form that will be shared after the event to mark your attendance.\n\nFor any queries or clarifications, please feel free to contact us.\n\nBest regards,\nAKSHAT KUMAR AND BUBLI BRAHMA  \nNSO Secretaries  \nIndian Institute of Technology, Bhilai  \nContact No. 7737288510, 6002846132\n\"\"\"\n\n# Generate and stream the summary\nprint(\"Generated Summary (streamed):\")\nfor partial_summary in generate_streaming(example_text):\n    pass  # The output is printed incrementally\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:41:03.923849Z","iopub.execute_input":"2025-01-20T19:41:03.924179Z","iopub.status.idle":"2025-01-20T19:41:23.273230Z","shell.execute_reply.started":"2025-01-20T19:41:03.924131Z","shell.execute_reply":"2025-01-20T19:41:23.272548Z"}},"outputs":[{"name":"stdout","text":"Generated Summary (streamed):\nThe Run for Wellbeing (R4W) event is a mandatory component of the NSO program for this semester. For students registering for two credits, 10 hours must be completed through participation in 10 R4W sessions, each session involving a 5 km run or walk, and attendance in a minimum of 10 sessions is mandatory. The remaining 18 hours need to be covered through other sports activities to guarantee an S grade for both credits. Failing to complete 5 sessions will result in unsatisfactory X grades in both credits, and passing all 10 sessions will guarantee satisfactory grades for both grades. Tracking and attendance are provided, and a Google form will be shared after the event to mark attendance.\r","output_type":"stream"}],"execution_count":5},{"id":"5f145a8b-91f0-498e-8dac-b3395bea4284","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}